# Generatively Pretrained Transformer
Custom implementation based of the paper Attention is All You Need to show interconnections and similarities between transformers and graph attention neural networks. The solution is derived from the original solution by Andrej Karpathy.
Reference: https://www.youtube.com/watch?v=kCc8FmEb1nY